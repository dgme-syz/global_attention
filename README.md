# global_attention

> **A Bad Try**

> [!IMPORTANT]
> æˆ‘ä»¬çš„ä»“åº“ä½¿ç”¨ [LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory) æä¾›çš„å¾®è°ƒæ”¯æŒï¼Œå¹¶ä¸”æ”¯æŒè¯¥å®éªŒæå‡ºçš„ `GA` æ–¹æ³•ï¼›ä½¿ç”¨ [peft](https://github.com/huggingface/peft) æä¾›çš„ `adapter` å¹¶ä¸”åˆ¶ä½œäº† `GA` adapter ä»è€Œèƒ½å¤Ÿåœ¨ LLMs ä¸Šæ·»åŠ æˆ‘ä»¬æ‰€éœ€è¦çš„æ¨¡å— (ğŸ“¢ ç°åœ¨å·²ç»å­˜åœ¨æ›´æ–¹ä¾¿çš„æ–¹æ¡ˆï¼Œå¯ä»¥å‚è€ƒ:[BenjaminBossan/peft](https://github.com/BenjaminBossan/peft/tree/refactor-peft-method-registration))ï¼›æœ€åï¼Œä½¿ç”¨ [lm-eval](https://github.com/EleutherAI/lm-evaluation-harness) å®Œæˆæ–°è®­ç»ƒæ¨¡å‹çš„è¯„æµ‹


åœ¨ `global_attention` æ–‡ä»¶å¤¹ä¸‹ï¼Œè¿è¡Œ `./scripts/train.sh` å³å¯æ‰§è¡Œå•å¡è®­ç»ƒã€‚

</br>

ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å®Œæˆåœ¨ `gsm_symbolic` æ•°æ®é›†çš„è¯„æµ‹ï¼š
```bash
lm_eval --model hf \
  --model_args pretrained=xxx,peft=xxx,dtype="float16" \
  --tasks gsm_symbolic \
  --device cuda:0 \
  --batch_size 4 \
  --output_path ./result/
```

> Releases ä¸­æœ‰ä¸€äº›æˆ‘ä»¬è®­ç»ƒå¥½çš„ **peft adapter**ï¼Œæ‚¨å¯èƒ½éœ€è¦åœ¨ `adapter_config.json` ä¸­ä¿®æ”¹ä¸€äº›ä¿¡æ¯ä»è€Œåœ¨æ‚¨çš„æœºå™¨ä¸Šè¿è¡Œ
